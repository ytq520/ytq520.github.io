<!DOCTYPE HTML>
<html>

<head><meta name="generator" content="Hexo 3.8.0">
	<link rel="bookmark" type="image/x-icon" href="/img/logo_miccall.png">
	<link rel="shortcut icon" href="/img/logo_miccall.png">
	
			    <title>
    ytq520
    </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <link rel="stylesheet" href="/css/mic_main.css">
    <link rel="stylesheet" href="/css/dropdownMenu.css">
    <meta name="keywords" content="miccall">
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css">
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css">
<link rel="stylesheet" href="/css/typo.css">
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">ytq520</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special">
            <ul class="menu links">
			<!-- Homepage  主页  --> 
			<li>
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/Data-Analysis/">Data Analysis</a></li><li><a class="category-link" href="/categories/Django/">Django</a></li><li><a class="category-link" href="/categories/Flask/">Flask</a></li><li><a class="category-link" href="/categories/JavaScript/">JavaScript</a></li><li><a class="category-link" href="/categories/MySQL/">MySQL</a></li><li><a class="category-link" href="/categories/html/">html</a></li><li><a class="category-link" href="/categories/python基础/">python基础</a></li><li><a class="category-link" href="/categories/spider/">spider</a>
	                    </li></ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/gallery/" title="图库">
		                图库
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/ytq520" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="500px" href="http://500px.com" target="_blank" rel="noopener">
                            <i class="icon fa fa-500px"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main">
            <div class="post_page_title_img" style="height: 25rem;background-image: url(/images/thumbs/19.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;">
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2>scrapy框架</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h4 id="scarpy框架"><a href="#scarpy框架" class="headerlink" title="scarpy框架"></a>scarpy框架</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">特点：爬取效率⾼、扩展性强、Python编写跨平台运⾏</span><br><span class="line">数据流程</span><br><span class="line">Scrapy中的数据流由执⾏引擎控制，其过程如下:</span><br><span class="line">1、引擎打开⼀个⽹站(open a domain)，找到处理该⽹站的Spider并向该spider请求第⼀个要</span><br><span class="line">爬取的URL(s)。</span><br><span class="line">2、引擎从Spider中获取到第⼀个要爬取的URL并在调度器(Scheduler)以Request调度。</span><br><span class="line">3、引擎向调度器请求下⼀个要爬取的URL。</span><br><span class="line">4、调度器返回下⼀个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)⽅向)转</span><br><span class="line">发给下载器(Downloader)。</span><br><span class="line">5、⼀旦⻚⾯下载完毕，下载器⽣成⼀个该⻚⾯的Response，并将其通过下载中间件(返回</span><br><span class="line">(response)⽅向)发送给引擎。</span><br><span class="line">6、引擎从下载器中接收到Response并通过Spider中间件(输入⽅向)发送给Spider处理。</span><br><span class="line">7、Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。</span><br><span class="line">8、引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度</span><br><span class="line">器。</span><br><span class="line">9、(从第⼆步)重复直到调度器中没有更多地request，引擎关闭该⽹站。</span><br><span class="line">安装Scrapy</span><br><span class="line">windows whl</span><br><span class="line">https://www.lfd.uci.edu/~gohlke/pythonlibs/</span><br><span class="line">pip install scrapy -i https://pypi.douban.com/simple</span><br><span class="line">windows 安装</span><br><span class="line">1.⾸先下载scrapy的whl包</span><br><span class="line">Python Extension Packages for Windows - Christoph Gohlke</span><br><span class="line">下载Scrapy-1.5.1-py2.py3-none-any.whl</span><br><span class="line">2.没有安装过wheel库的请先安装pip install wheel</span><br><span class="line">3.scrapy依赖twiste，进⼊ http://www.lfd.uci.edu/~gohlke/pythonlibs/ ，找到适合的版本，</span><br><span class="line">下载Twisted-18.9.0-cp37-cp37m-win_amd64.whl</span><br><span class="line">4.scrapy依赖lxml包，pip install lxml</span><br><span class="line">5.在下载存放的⽬录下安装</span><br><span class="line">pip install Twisted-18.9.0-cp37-cp37m-win_amd64.whl</span><br><span class="line">如遇到需要下载 pywin32，请下载安装</span><br><span class="line">Python for Windows Extensions - Browse /pywin32/Build 221 at SourceForge.net</span><br><span class="line">pip install pypiwin32</span><br><span class="line">6.在下载存放的⽬录下安装pip install scrapy</span><br><span class="line">### 创建项⽬</span><br><span class="line">scrapy startproject myscrapy</span><br><span class="line">防⽌被爬⽹站的robots.txt起作⽤</span><br><span class="line">settings.py</span><br><span class="line">修改 ROBOTSTXT_OBEY = False</span><br><span class="line">### 创建Spider</span><br><span class="line">scrapy genspider football sports.sina.com.cn</span><br><span class="line">Spider 属性</span><br><span class="line">name: Spider名字</span><br><span class="line">allowed_domains: 允许爬取的域名</span><br><span class="line">start_urls: Spider启动时爬取的url列表</span><br><span class="line">parse: 负责解析返回的响应，提取数据或进⼀步处理</span><br><span class="line">创建Item</span><br><span class="line">Item是保存爬取数据的容器</span><br><span class="line">解析Response</span><br><span class="line">使⽤Item</span><br><span class="line">后续Request</span><br><span class="line">运⾏</span><br><span class="line">scrapy crawl football</span><br><span class="line">保存到⽂件 (csv, xml, pickle, marshal)</span><br><span class="line">scrapy crawl football -o football.json</span><br></pre></td></tr></table></figure>
<p>###连接并写入数据库在setting最后写入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#将67行取消注释</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   &apos;qichamao.pipelines.QichamaoMysqlPipeline&apos;: 300,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">FEED_EXPORT_ENCODING = &apos;utf-8&apos;</span><br><span class="line">MAX_PAGES = 200</span><br><span class="line"></span><br><span class="line"># mysql settings</span><br><span class="line">MYSQL_HOST = &apos;127.0.0.1&apos;</span><br><span class="line">MYSQL_PORT = 3306</span><br><span class="line">MYSQL_USERNAME = &apos;root&apos;</span><br><span class="line">MYSQL_PASSWORD = &apos;123456&apos;</span><br><span class="line">MYSQL_DATABASE = &apos;qichamao&apos;</span><br></pre></td></tr></table></figure></p>
<p>###在pipelines.py中添加并写成如下代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class QichamaoMysqlPipeline(object):</span><br><span class="line">    def __init__(self, host, port, database, username, password):</span><br><span class="line">        self.host = host</span><br><span class="line">        self.port = port</span><br><span class="line">        self.database = database</span><br><span class="line">        self.username = username</span><br><span class="line">        self.password = password</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            host=crawler.settings.get(&apos;MYSQL_HOST&apos;),</span><br><span class="line">            port=crawler.settings.get(&apos;MYSQL_PORT&apos;),</span><br><span class="line">            database=crawler.settings.get(&apos;MYSQL_DATABASE&apos;),</span><br><span class="line">            username=crawler.settings.get(&apos;MYSQL_USERNAME&apos;),</span><br><span class="line">            password=crawler.settings.get(&apos;MYSQL_PASSWORD&apos;),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        # 获取数据库连接</span><br><span class="line">        self.db = pymysql.connect(self.host, self.username, self.password, self.database, charset=&apos;utf8&apos;, port=self.port)</span><br><span class="line">        self.cursor = self.db.cursor()</span><br><span class="line"></span><br><span class="line">    def class_spider(self, spider):</span><br><span class="line">        # 释放数据库连接</span><br><span class="line">        self.db.close()</span><br><span class="line"></span><br><span class="line">    # 插入item数据</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        sql = &apos;insert into company (company_name, company_code, company_desc, logo_url, c_name, c_phone, c_position, c_email) values (%s,%s,%s,%s,%s,%s,%s,%s)&apos;</span><br><span class="line">        self.cursor.execute(sql, (item[&apos;company_name&apos;], item[&apos;company_code&apos;], item[&apos;company_desc&apos;], item[&apos;logo_url&apos;], item[&apos;c_name&apos;], item[&apos;c_phone&apos;], item[&apos;c_position&apos;], item[&apos;c_email&apos;]))</span><br><span class="line">        self.db.commit()</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure></p>
<p>###在items.py和django的models用法一致<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># Define here the models for your scraped items</span><br><span class="line">#</span><br><span class="line"># See documentation in:</span><br><span class="line"># https://doc.scrapy.org/en/latest/topics/items.html</span><br><span class="line"></span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class QichamaoItem(scrapy.Item):</span><br><span class="line">    # define the fields for your item here like:</span><br><span class="line">    company_name = scrapy.Field()</span><br><span class="line">    company_code = scrapy.Field()</span><br><span class="line">    company_desc = scrapy.Field()</span><br><span class="line">    logo_url = scrapy.Field()</span><br><span class="line">    c_name = scrapy.Field()</span><br><span class="line">    c_phone = scrapy.Field()</span><br><span class="line">    c_position = scrapy.Field()</span><br><span class="line">    c_email = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    pass</span><br></pre></td></tr></table></figure></p>
<p>###爬取企查猫的数据并保存数据库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">import json</span><br><span class="line">from qichamao.items import QichamaoItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class CatSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;cat&apos;</span><br><span class="line">    allowed_domains = [&apos;www.qichamao.com&apos;]</span><br><span class="line">    start_urls = [&apos;https://www.qichamao.com/cert-wall/&apos;]</span><br><span class="line"></span><br><span class="line">    # 负责构造请求</span><br><span class="line">    def start_requests(self):</span><br><span class="line">        data = &#123;&apos;pagesize&apos;: &apos;9&apos;&#125;</span><br><span class="line">        headers = &#123;</span><br><span class="line">            &apos;Referer&apos;: &apos;https://www.qichamao.com/cert-wall/&apos;,</span><br><span class="line">            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3610.2 Safari/537.36&apos;,</span><br><span class="line">            &apos;Host&apos;: &apos;www.qichamao.com&apos;,</span><br><span class="line">            &apos;Accept&apos;: &apos;application/json, text/plain, */*&apos;,</span><br><span class="line">            &apos;Accept-Encoding&apos;: &apos;gzip, deflate, sdch&apos;,</span><br><span class="line">            &apos;Accept-Language&apos;: &apos;zh-CN,zh;q=0.8,en;q=0.6,ja;q=0.4,zh-TW;q=0.2,mt;q=0.2&apos;,</span><br><span class="line">            &apos;Connection&apos;: &apos;keep-alive&apos;,</span><br><span class="line">            &apos;X-Requested-With&apos;: &apos;XMLHttpRequest&apos;,</span><br><span class="line">        &#125;</span><br><span class="line">        max_page = self.settings.get(&apos;MAX_PAGES&apos;)</span><br><span class="line">        base_url = &apos;https://www.qichamao.com/cert-wall/&apos;</span><br><span class="line">        for page in range(2, max_page):</span><br><span class="line">            data[&apos;page&apos;] = str(page)</span><br><span class="line">            yield scrapy.FormRequest(url=base_url,</span><br><span class="line">                                     headers=headers,</span><br><span class="line">                                     method=&apos;POST&apos;,</span><br><span class="line">                                     formdata=data,</span><br><span class="line">                                     callback=self.parse)</span><br><span class="line"></span><br><span class="line">    # 负责处理请求</span><br><span class="line">    def parse(self, response):</span><br><span class="line">        result_json = json.loads(response.text)</span><br><span class="line">        data_list = result_json[&apos;dataList&apos;]</span><br><span class="line">        for data in data_list:</span><br><span class="line">            item = QichamaoItem()</span><br><span class="line">            item[&apos;company_name&apos;] = data[&apos;CompanyName&apos;]</span><br><span class="line">            item[&apos;company_code&apos;] = data[&apos;CompanyCode&apos;]</span><br><span class="line">            item[&apos;company_desc&apos;] = data[&apos;CompanyBrief&apos;]</span><br><span class="line">            item[&apos;logo_url&apos;] = data[&apos;logoUrl&apos;]</span><br><span class="line">            item[&apos;c_name&apos;] = data[&apos;c_name&apos;]</span><br><span class="line">            item[&apos;c_phone&apos;] = data[&apos;c_phone&apos;]</span><br><span class="line">            item[&apos;c_position&apos;] = data[&apos;c_position&apos;]</span><br><span class="line">            item[&apos;c_email&apos;] = data[&apos;c_email&apos;]</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure></p>

            </div>

            <!-- Post Comments -->
            

        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
